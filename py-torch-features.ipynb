{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\n\nn1=  torch.tensor([\n    [[2,1,1,1],\n    [1,1,1,1],\n    [1,1,1,1]],\n\n    [[1,1,1,1],\n    [1,1,1,1],\n    [1,1,1,9]],\n])\n\n\nprint(f\"{n1.ndim=}, {n1.shape=}, {n1.dtype=}\\n\")\n\nones = torch.ones(2,2)\nzeros = torch.zeros(3,3)\narange = torch.arange(1,10,3)\nlinspace = torch.linspace(1,10,3)\nrand= torch.rand(2,2)\n\nones = torch.ones(3, 2)\ntranspose = torch.transpose(ones, 0, 1)  # Swap dim 0 and dim 1\n\n\n# Create a tensor with random integers from 0 to 9 (exclusive), shape (2, 3)\nrand_int = torch.randint(low=0, high=10, size=(2, 3))\n\nprint(f\"a:\\n {ones}\\n\\n b:\\n {zeros}\\n\\n c:\\n {arange}\\n\\n\")\n\nprint(f\"linspace: \\n {linspace}\\n\\n\")\n\n\nprint(f\"randint: \\n {rand_int}\\n\\n\")\n\nprint(f\"Before transponse:\\n{ones}\\n\\n\")\n\nprint(f\"{transpose:}\\n\\n\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-26T21:25:18.316315Z","iopub.execute_input":"2025-05-26T21:25:18.316782Z","iopub.status.idle":"2025-05-26T21:25:18.326395Z","shell.execute_reply.started":"2025-05-26T21:25:18.316762Z","shell.execute_reply":"2025-05-26T21:25:18.325799Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch.nn as nn\n\ntorch.nn.Linear(in_features, out_features, bias=True),\ntorch.nn.Conv2d(in_channels, out_channels, kernel_size)\ntorch.nn.MaxPool2d(kernel_size)\n\n#Activation Functions\ntorch.nn.Relu() \ntorch.nn.Sigmoid()\ntorch.nn.tanh()\ntorch.nn.Softmax()\n\n# Loss Functions\ntorch.nn.MSELoss() , torch.nn.L1Loss() , torch.nn.CrossEntropyLoss() , torch.nn.BCELoss() , torch.nn.NLLLoss() , torch.nn.KLDivLoss()\n\n\n# Normalization\ntorch.nn.BatchNorm1d(num_features)\ntorch.nn.BatchNorm2d(num_features)\ntorch.nn.LayerNorm(normalized_shape)\ntorch.nn.GroupNorm(num_groups, num_channels)\n\n\n# Recurrent layers\ntorch.nn.RNN(input_size, hidden_size, num_layers, batch_first)\ntorch.nn.LSTM(input_size, hidden_size, num_layers, batch_first)\ntorch.nn.GRU(input_size, hidden_size, num_layers, batch_first)\n\n# Attention layers\ntorch.nn.MultiheadAttention(embed_dim, num_heads, dropout=0.0)\n\n# Transformer layer\ntorch.nn.Transformer(d_model, nhead, num_encoder_layers, num_decoder_layers)\n\n# Dropout\ntorch.nn.Dropout(p)\n\n# Embedding layer\ntorch.nn.Embedding(num_embeddings, embedding_dim)\n\n\n\n\n# Torch.optim\n\ntorch.optim.SGD(params, lr=0.01, momentum=0.0, dampening=0, weight_decay=0, nesterov=False)\ntorch.optim.Adam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\ntorch.optim.RMSprop(params, lr=0.01, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False)\ntorch.optim.Adadelta(params, lr=1.0, rho=0.9, eps=1e-06, weight_decay=0)\ntorch.optim.Adagrad(params, lr=0.01, lr_decay=0, weight_decay=0, initial_accumulator_value=0, eps=1e-10)\n\n# Learning Rate\n\ntorch.optim.lr_scheduler.StepLR(optimizer, step_size, gamma=0.1, last_epoch=-1)\ntorch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08, verbose=False)\ntorch.optim.lr_scheduler.consine_annealing_lr(optimizer, T_max, eta_min=0, last_epoch=-1)\n\n#Torch AutoGrad\ntorch.autograd.backward(tensors, grad_tensors=None, retain_graph=None, create_graph=False, grad_variables=None)\ntorch.autograd.grad(outputs, inputs, grad_outputs=None, retain_graph=None, create_graph=False, only_inputs=True, allow_unused=False)\ntorch.autograd.gradcheck(func, inputs, eps=1e-07, atol=1e-07, rtol=1e-07)\ntorch.autograd.gradmode(mode)\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T21:26:21.613990Z","iopub.execute_input":"2025-05-26T21:26:21.614724Z","iopub.status.idle":"2025-05-26T21:26:21.629290Z","shell.execute_reply.started":"2025-05-26T21:26:21.614701Z","shell.execute_reply":"2025-05-26T21:26:21.628479Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\n\n# Create a tensor with shape (1, 3, 1, 4)\nx = torch.randint(0, 10, (1, 3, 1, 4))\nprint(\"Original Tensor:\\n\", x)\nprint(\"Original Shape:\", x.shape)\n\n# Squeeze the tensor\nx_squeezed = torch.squeeze(x)\nprint(\"\\nSqueezed Tensor:\\n\", x_squeezed)\nprint(\"Squeezed Shape:\", x_squeezed.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T21:40:20.113584Z","iopub.execute_input":"2025-05-26T21:40:20.114147Z","iopub.status.idle":"2025-05-26T21:40:20.121066Z","shell.execute_reply.started":"2025-05-26T21:40:20.114123Z","shell.execute_reply":"2025-05-26T21:40:20.120363Z"}},"outputs":[],"execution_count":null}]}