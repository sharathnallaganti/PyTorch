{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNzFWmhWfJXxwvvQimFNJOu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sharathnallaganti/PyTorch/blob/main/PyTorch_Modules.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jxZh3ioBq-nT"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "  torch.tensor, torch.add(), torch.sub(),torch.mul(), torch.div(), torch.pow(), torch.sqrt(), torch.ones(), torch.zeros(), torch.rand\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import torch.nn as nn\n",
        "   torch.nn.Linear(in_features, out_features, bias=True),\n",
        "   torch.nn.Conv2d(in_channels, out_channels, kernel_size)\n",
        "   torch.nn.MaxPool2d(kernel_size)\n",
        "\n",
        "#Activation Functions\n",
        "torch.nn.Relu() , torch.nn.Sigmoid(), torch.nn.tanh(), torch.nn.Softmax()\n",
        "\n",
        "# Loss Functions\n",
        "torch.nn.MSELoss() , torch.nn.L1Loss() , torch.nn.CrossEntropyLoss() , torch.nn.BCELoss() , torch.nn.NLLLoss() , torch.nn.KLDivLoss()\n",
        "\n",
        "\n",
        "# Normalization\n",
        "torch.nn.BatchNorm1d(num_features)\n",
        "torch.nn.BatchNorm2d(num_features)\n",
        "torch.nn.LayerNorm(normalized_shape)\n",
        "torch.nn.GroupNorm(num_groups, num_channels)\n",
        "\n",
        "\n",
        "# Recurrent layers\n",
        "torch.nn.RNN(input_size, hidden_size, num_layers, batch_first)\n",
        "torch.nn.LSTM(input_size, hidden_size, num_layers, batch_first)\n",
        "torch.nn.GRU(input_size, hidden_size, num_layers, batch_first)\n",
        "\n",
        "# Attention layers\n",
        "torch.nn.MultiheadAttention(embed_dim, num_heads, dropout=0.0)\n",
        "\n",
        "# Transformer layer\n",
        "torch.nn.Transformer(d_model, nhead, num_encoder_layers, num_decoder_layers)\n",
        "\n",
        "# Dropout\n",
        "torch.nn.Dropout(p)\n",
        "\n",
        "# Embedding layer\n",
        "torch.nn.Embedding(num_embeddings, embedding_dim)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Torch.optim\n",
        "\n",
        "torch.optim.SGD(params, lr=0.01, momentum=0.0, dampening=0, weight_decay=0, nesterov=False)\n",
        "torch.optim.Adam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
        "torch.optim.RMSprop(params, lr=0.01, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False)\n",
        "torch.optim.Adadelta(params, lr=1.0, rho=0.9, eps=1e-06, weight_decay=0)\n",
        "torch.optim.Adagrad(params, lr=0.01, lr_decay=0, weight_decay=0, initial_accumulator_value=0, eps=1e-10)\n",
        "\n",
        "# Learning Rate\n",
        "\n",
        "torch.optim.lr_scheduler.StepLR(optimizer, step_size, gamma=0.1, last_epoch=-1)\n",
        "torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08, verbose=False)\n",
        "torch.optim.lr_scheduler.consine_annealing_lr(optimizer, T_max, eta_min=0, last_epoch=-1)\n",
        "\n",
        "#Torch AutoGrad\n",
        "torch.autograd.backward(tensors, grad_tensors=None, retain_graph=None, create_graph=False, grad_variables=None)\n",
        "torch.autograd.grad(outputs, inputs, grad_outputs=None, retain_graph=None, create_graph=False, only_inputs=True, allow_unused=False)\n",
        "torch.autograd.gradcheck(func, inputs, eps=1e-07, atol=1e-07, rtol=1e-07)\n",
        "torch.autograd.gradmode(mode)\n",
        "\n",
        "\n",
        "# Torch.util.data\n",
        "torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False, sampler=None, batch_sampler=None, num_workers=0, collate_fn)\n",
        "torch.utils.data.Dataset(samples)\n",
        "\n",
        "\n",
        "#"
      ]
    }
  ]
}