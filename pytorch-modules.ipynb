{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\n  torch.tensor, torch.add(), torch.sub(),torch.mul(), torch.div(), torch.pow(), torch.sqrt(), torch.ones(), torch.zeros(), torch.rand\n\n\n\n\nimport torch.nn as nn\n   torch.nn.Linear(in_features, out_features, bias=True),\n   torch.nn.Conv2d(in_channels, out_channels, kernel_size)\n   torch.nn.MaxPool2d(kernel_size)\n\n#Activation Functions\ntorch.nn.Relu() , torch.nn.Sigmoid(), torch.nn.tanh(), torch.nn.Softmax()\n\n# Loss Functions\ntorch.nn.MSELoss() , torch.nn.L1Loss() , torch.nn.CrossEntropyLoss() , torch.nn.BCELoss() , torch.nn.NLLLoss() , torch.nn.KLDivLoss()\n\n\n# Normalization\ntorch.nn.BatchNorm1d(num_features)\ntorch.nn.BatchNorm2d(num_features)\ntorch.nn.LayerNorm(normalized_shape)\ntorch.nn.GroupNorm(num_groups, num_channels)\n\n\n# Recurrent layers\ntorch.nn.RNN(input_size, hidden_size, num_layers, batch_first)\ntorch.nn.LSTM(input_size, hidden_size, num_layers, batch_first)\ntorch.nn.GRU(input_size, hidden_size, num_layers, batch_first)\n\n# Attention layers\ntorch.nn.MultiheadAttention(embed_dim, num_heads, dropout=0.0)\n\n# Transformer layer\ntorch.nn.Transformer(d_model, nhead, num_encoder_layers, num_decoder_layers)\n\n# Dropout\ntorch.nn.Dropout(p)\n\n# Embedding layer\ntorch.nn.Embedding(num_embeddings, embedding_dim)\n\n\n\n\n# Torch.optim\n\ntorch.optim.SGD(params, lr=0.01, momentum=0.0, dampening=0, weight_decay=0, nesterov=False)\ntorch.optim.Adam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\ntorch.optim.RMSprop(params, lr=0.01, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False)\ntorch.optim.Adadelta(params, lr=1.0, rho=0.9, eps=1e-06, weight_decay=0)\ntorch.optim.Adagrad(params, lr=0.01, lr_decay=0, weight_decay=0, initial_accumulator_value=0, eps=1e-10)\n\n# Learning Rate\n\ntorch.optim.lr_scheduler.StepLR(optimizer, step_size, gamma=0.1, last_epoch=-1)\ntorch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08, verbose=False)\ntorch.optim.lr_scheduler.consine_annealing_lr(optimizer, T_max, eta_min=0, last_epoch=-1)\n\n#Torch AutoGrad\ntorch.autograd.backward(tensors, grad_tensors=None, retain_graph=None, create_graph=False, grad_variables=None)\ntorch.autograd.grad(outputs, inputs, grad_outputs=None, retain_graph=None, create_graph=False, only_inputs=True, allow_unused=False)\ntorch.autograd.gradcheck(func, inputs, eps=1e-07, atol=1e-07, rtol=1e-07)\ntorch.autograd.gradmode(mode)\n\n\n# Torch.util.data\ntorch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False, sampler=None, batch_sampler=None, num_workers=0, collate_fn)\ntorch.utils.data.Dataset(samples)\n\n\n#","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]}]}